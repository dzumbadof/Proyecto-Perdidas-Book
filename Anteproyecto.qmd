
```{r echo=FALSE}
#| output: false
#| warning: false
library(readxl)
library(kableExtra)
library(readr)
library(dplyr)
library(janitor) # función clean_names()
library(magrittr) # función %<>%
library(stringr) # función str_replace
library(lubridate)
library(PerformanceAnalytics) # funciones skewness y kurtosis
library(ggplot2)
library(actuar)
library(fitdistrplus)
library(stats)
library(cowplot) # mejorar el aspecto de los gráficos
library(xtable) # tablas a latex
```

# Pregunta de investigación

¿Cómo se puede modelar las pérdidas ligadas a los daños a la propiedad y a las personas en aeropuertos a partir de su frecuencia y severidad?

# Objetivos

## Objetivo general

- Modelar la distribución de las pérdidas ligadas a daños a la propiedad y a las personas en aeropuertos estadounidenses con base en datos de la Administración de Seguridad en el Transporte (TSA) para el período 2010-2013.

## Objetivos específicos

- Elegir la distribución de frecuencia de reclamos de daños a la propiedad y a las personas en aeropuertos estadounidenses con base en datos de TSA para el período 2010-2013.

- Escoger la distribución de la severidad de reclamos de daños a la propiedad y a las personas en aeropuertos estadounidenses con base en datos de TSA para el período 2010-2013.

- Construir la distribución de pérdidas totales ligadas a los daños a la propiedad y a las personas en aeropuertos estadounidenses a partir de las distribuciones de frecuencia y severidad y con base en datos de TSA para el período 2010-2013.

# Marco teórico 


Según  @RAE, un modelo es un esquema teórico, generalmente en forma matemática, de un sistema o de una realidad compleja, que se elabora para facilitar la compresión y el estudio de un comportamiento. En el caso particular en que se busque modelar las pérdidas asociadas a un conjunto de eventos o sucesos, se le denomina modelo de pérdidas.

 @soadefinitions establece que las pérdidas dependen de dos variables aleatorias. La primera es el número de pérdidas que se producirán en un periodo determinado. Esta variable aleatoria del número de pérdidas se denomina comúnmente frecuencia de pérdidas y su distribución de probabilidad se llama distribución de frecuencia.
 
La segunda variable aleatoria es el monto de la pérdida, dado que una pérdida ha ocurrido. Este monto suele denominarse severidad, y la distribución de probabilidad para el monto de la pérdida se denomina distribución de severidad.

En este trabajo, se busca modelar las pérdidas ligadas a los daños a la propiedad y a las personas en aeropuertos de Estados Unidos para el período 2010-2013 , a partir de la frecuencia y severidad de estos.
 Con la finalidad de contextualizar nuestro estudio, es importante aclarar que el TSA (*Transportation Security Administration*) es la agencia establecida luego del 2001 que se ocupa del chequeo de los pasajeros y su equipaje en los aeropuertos de Estados Unidos.

Como consecuencia de sus labores, es común que se causen daños y extravíos de las pertenencias de los pasajeros lo que resulta en reclamos por parte de los mismos en la forma de compensación monetaria por los daños ocasionados. 

## Teoría de estimación paramétrica de modelos de frecuencia y severidad vía máxima verosimilitud.

Existen una serie de distribuciones de probabilidad estándar que se podrían utilizar para aproximar las distribuciones de las variables aleatorias de la frecuencia de reclamaciones y la severidad o monto de estos reclamos. Las distribuciones binomial, geométrica, binomial negativa y Poisson se consideran para la modelización de la frecuencia.

Por otro lado, entre las distribuciones estándar para modelar la severidad se tienen las siguientes distribuciones: exponencial, gamma, Weibull, Pareto y lognormal.

Tal como lo establece  @cyprian una forma de abordar la escogencia de la distribución correcta es ajustando los datos a las distribuciones estadísticas seleccionadas y los parámetros se estiman mediante el método de máxima verosimilitud.

## Teoría de pruebas de bondad de ajuste.

Las pruebas de bondad de ajuste son utilizadas de forma recurrente una vez ajustadas las distribuciones a los datos y estimados los parámetros respectivos con la finalidad de elegir entre las distribuciones que compiten entre sí.

Una prueba de bondad de ajuste es “un procedimiento estadístico que describe qué tan bien se ajusta una distribución a un conjunto de observaciones mediante la medición de la compatibilidad cuantificable entre las distribuciones teóricas estimadas y la distribución empírica de los datos muestrales” [@cyprian]. Estas pruebas adoptan la estructura de prueba de hipótesis donde la hipótesis nula consiste en que los datos siguen una distribución particular. Se presenta a continuación una idea general de tres pruebas de bondad de ajuste de uso frecuente:

- Prueba Chi-Cuadrado: Esta prueba propone un estadístico compuesto de frecuencias observadas y esperadas, calculado a partir de una partición de la muestra, el cual presenta bajo la hipótesis nula una distribución Chi-Cuadrado con grados de libertad que dependen de la cantidad de datos, la cantidad de intervalos de la partición y la cantidad de parámetros de la distribución propuesta calculados por medio de los datos muestrales.

- Prueba Kolmogorov-Smirnov: Se basa en comparar la función de distribución propuesta con la función de distribución empírica de los datos para medir el ajuste, partiendo de que la función de distribución caracteriza a una distribución de probabilidad. Esta comparación se realiza mediante un estadístico que mide la distancia entre ambas distribuciones, del cual se conocen ciertos resultados de convergencia y distribución que fundamentan la efectividad del método.

- Prueba Anderson-Darling:
Se asemeja a la de Kolmogorov-Smirnov pero mide de una forma distinta la diferencia entre las funciones de distribución empírica y teórica. Además, de acuerdo a @klugman2019loss el estadístico de prueba de Anderson-Darling suele priorizar un mejor ajuste en las colas de la distribución en comparación con las regiones más centrales.







# Análisis descriptivo 


```{r}
#| output: false
#| warning: false
#| echo: false

# Se cargan y depuran los datos

datos <- read_excel("claims-2010-2013.xlsx") %>%
  clean_names() # se limpian nombres columnas

datos <- datos %>% mutate(date_received = ymd(date_received),
                      close_amount = as.numeric(gsub("\\$", "", close_amount)))

# Se fija la base de datos

attach(datos)
```


```{r}
#| tbl-cap: "Reclamos recibidos por la TSA en el periodo 2010-2013"
#| label: tbl-headDatos
#| echo: false


head(datos) %>%
  kbl(align = rep('r', ncol(head(datos))),
      format.args = list(big.mark = ' '),booktabs=T) %>% 
  kable_styling(latex_options = c("striped", "condensed", "scale_down"))  %>%
  kable_styling(full_width = F) %>% kable_classic_2() %>%
  row_spec(0,bold=TRUE) %>% 
  column_spec(1:11, width = "3cm") %>% 
  scroll_box()

# print(xtable(head(datos), digits = 0,
#              caption = "Reclamos recibidos por la TSA en el periodo 2010-2013", 
#              align = "|l|l|l|l|l|l|l|l|l|l|l|l|" , 
#              label = "headDatos"),
#       include.rownames = F, 
#       include.colnames = T, 
#        caption.placement = "top", 
#       row.names=F)


```

En este punto se observa que la variable de mayor interés es *close_amount*, pues corresponde al desembolso efectivo al atender reclamos. Sin embargo, esta variable no es en sí misma útil para implementar los modelos sugeridos, sino que se tienen que construir los datos de frecuencia y severidad de los reclamos a TSA. Siguiendo a @flores y @chen2020aggregate se realizan dos cambios relevantes a este respecto. El primero consiste en filtrar la base de datos para conservar solamente aquellas observaciones en que efectivamente hubo un desembolso al atender el reclamo. Para esto se utiliza la variable *disposition*, que corresponde al estado de resolución del reclamo e indica si el reclamo fue denegado, si se pagó por completo el monto solicitado (aprobado) o si se llegó a un acuerdo (acordado) y se pagó solamente una fracción del monto del reclamo. Consecuentemente, al filtrar las observaciones se pasa de 41 598 observaciones a 12 743

```{r, message=FALSE, warning=FALSE}
#| echo: false

# X: Severidad
# N: Frecuencia

# Se filtran los reclamos aprobados o en los que se llegó a un acuerdo

datos_agregados <- datos %>% filter(disposition == "Settle" | disposition== "Approve in Full")

datos_agregados <- datos_agregados %>% group_by("ano" = year(date_received), "mes" = month(date_received) ) %>%  summarise(X = sum(close_amount), N = n()) %>% 
  ungroup() %>% mutate(t = c(1:48), .before = X)
```

En la @tbl-medidas_severidad_frecuencia se muestran algunas estadísticas de los datos de frecuencia y severidad. Sorprende principalmente la asimetría obtenida para la severidad, que marca una discrepancia con los resultados obtenidos tanto por @flores como por @chen2020aggregate, dado que ambos autores presentan coeficientes de asimetría positivos, sin embargo, debe tenerse en cuenta que el primero utiliza datos del período 2003-2015 (desagregados además por sitio y tipo) y el segundo del período 2008-2012. De la @fig-histograma_severidad ya se observaba que no hay una asimetría positiva marcada en la severidad.


```{r, echo=FALSE}
#| tbl-cap: "Medidas estadísticas de resumen para la severidad y la frecuencia"
#| label: tbl-medidas_severidad_frecuencia

medidas <- function(x){
  r <- summary(x) %>% as.vector()
  temp <- data.frame(c(r, sd(x), IQR(x),skewness(x), kurtosis(x)))
  return(temp)
}


nombres <- c("Mínimo",
             "Primer cuartil",
             "Mediana",
             "Media",
             "Tercer cuartil",
             "Máximo",
             "Desviación estándar",
             "Rango intercuartil",
             "Asimetría",
             "Curtosis")

tabla <- cbind(medidas(datos_agregados$N), medidas(datos_agregados$X))

rownames(tabla) <- nombres
colnames(tabla) <- c("Frecuencia", "Severidad")

tabla %>%  kbl(digits = 2) %>%
  kable_styling() %>%
  kable_classic_2(full_width = F) %>% 
   row_spec(0, bold = T) %>% 
  column_spec(c(1,2,3), width = "2cm") %>%
    column_spec(1, bold = T) %>% t()

print(xtable(t(tabla), digits = 2,
             caption = "Medidas estadísticas de resumen para la severidad y la frecuencia",
             label = "medidas_severidad_frecuencia"),
      include.rownames = T,
      include.colnames = T,
       caption.placement = "top",
      row.names=F)

```

```{r}
#| fig-cap: "Número de reclamos mensuales del 2010 al 2013"
#| label: fig-reclamosmensuales
#| echo: false


ggplot(datos_agregados, aes( x=t, y = N)) + geom_point(color='red', size=2) + 
  xlab("Tiempo") + ylab("Reclamos")+
  theme_minimal()
```

La @fig-reclamosmensuales muestra el conteo de incidencias menusales entre 2010 y finales del 2013. Se Muestra una tendencia fuerte de incrementeo hasta el mes 40. Esto se puede explicar a partir de que el TSA fue creado en el 2002 y durante su período inicial de funcionamiento se implementaron nuevas prácticas de seguridad en el aeropuerto por lo que los pasajeros y las autoridades tuvieron un período de aprendizaje. Luego del mes 40 se observa una fuerte tendencia de decremento posiblemente porque la población a este punto ya se acostumbró a las nuevas medidas implementadas. Esta tendencia es importante notarla pues @flores comenta que puede dificultar el proceso de ajustarle una distribución. 
```{r, echo=FALSE}
#| fig-cap: "Histograma de montos pagados agregados por mes"
#| label: fig-histograma_severidad

g <- ggplot(datos_agregados, aes(x=X)) 
g <- g + geom_histogram(colour="black", fill="#40A195", bins=7)
g <- g + scale_y_continuous(breaks = seq(0,22,2))
g <- g + scale_x_continuous(breaks = seq(5000,125000, 15000))
g <- g + labs(x = "Monto pagado",
              y = "Frecuencia",
              caption = "Fuente: Elaboración propia con datos de TSA")

g <- g + theme_cowplot()

g

```

En la @fig-histograma_severidad se observa la distribución empírica de la severidad. Se observa que la cola izquierda aparenta acumular un mayor peso que la derecha y que la mayor concentración ocurre aproximadamente para los montos pagados entre 50 000 y 60 000 dólares.


# Propuesta de métodos para responder la pregunta de investigación

Para responder al primer objetivo de la investigación, se proponen distintos acercamientos paramétricos para modelar la frecuencia, que se pueden resumir en los integrantes de la clase distribuciones discretas $(a,b,0)$, a saber: la distribución binomial, la binomial negativa, la geométrica (que es un caso particular de la biniomial negativa) y la Poisson. Además, se considerarán a su vez los integrantes de la familia $(a,b,1)$, que corresponden a variaciones de las anteriores con modificaciones o truncamientos en los valores de la probabilidad de ocurrencia de cero reclamos. 

En cuanto a la estimación de los parámetros de los modelos de frecuencia, estos sea realizarán vía máxima verosimilitud. Posteriormente, para decidir cuál distribución es más adecuada para modelizar la frecuencia de los reclamos, se conducirá sobre cada una una prueba chi cuadrado de bondad de ajuste. Una vez obtenidas las distribuciones para las que no se haya obtenido un rechazo en la prueba anterior, se utilizará la magnitud de las medidas de AIC y BIC para comparar entre las distintas opciones y decidir la más adecuada. 

Para responder al segundo objetivo, se proponen acercamientos paramétricos mediante las distribuciones: Exponencial, Gamma, Pareto, Lognormal y Weibull. De forma similar al caso de la frecuencia, con la severidad se conducirán pruebas de bondad de ajuste mediante las técnicas de Kolmogorov-Smirnov y Anderson-Darling. De las distribuciones para las que no se rechazara antes la bondad de ajuste, se determinará la más popicia con las medidas de AIC y BIC.

Finalmente, bajo un modelo de pérdidas agregadas, se buscará determinar la distribución de las pérdidas totales en dos pasos: 1) Se procede a discretizar la distribución de la severidad mediante el método de redondeo; y 2) Se calcula la distribución de las pédidas totales mediante la fórmula de Panjer con la distribución de frecuencia seleccionada y la de severidad discretizada. 


