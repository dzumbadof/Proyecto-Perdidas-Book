---
editor: 
  markdown: 
    wrap: 72
---

# Bitácora 4

```{r, setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
options(scipen = 99, digits = 4)

```

```{r echo=FALSE, message=FALSE, warning=TRUE}
#| output: false
#| warning: false

library(readxl)
library(kableExtra)
library(readr)
library(dplyr)
library(janitor) # función clean_names()
library(magrittr) # función %<>%
library(stringr) # función str_replace
library(lubridate)
library(PerformanceAnalytics) # funciones skewness y kurtosis
library(ggplot2)
library(actuar)
library(fitdistrplus)
library(stats)
library(cowplot) # mejorar el aspecto de los gráficos
library(gamlss) #distribucion Johnson SU
library(glogis) #distribucion logistica generalizada

#Valores Extremos
library(extRemes)
library(evd)
```

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| warning: false

datos <- read_excel("claims-2010-2013.xlsx") %>%

clean_names() # se limpian nombres columnas

datos <- datos %>% mutate(date_received = ymd(date_received),

close_amount = as.numeric(gsub("\\$", "", close_amount)))

# Se fija la base de datos

attach(datos)
```

```{r, message=FALSE, warning=FALSE}
#| echo: false

# X: Severidad

# N: Frecuencia

# Se filtran los reclamos aprobados o en los que se llegó a un acuerdo

datos_agregados <- datos %>% 
  filter(disposition == "Settle" | disposition== "Approve in Full")

datos_agregados <- datos_agregados %>% 
  group_by("ano" = year(date_received), "mes" = month(date_received) ) %>% 
  summarise(X = sum(close_amount), N = n(), mean_sev=mean(close_amount)) %>%
  ungroup() %>% mutate(t = c(1:48), .before = X)

```

## Parte de planificación

### Variciones en el ajuste de la frecuencia

En la bitácora anterior se comentó que se valoraba probar con distribuciones compuestas para mejorar el ajuste de la frecuencia, buscando con estas una mayor flexibilidad. En ese sentido se probó con las composiciones Poisson-Gaussiana inversa, Poisson-Geométrica (Distribución Polya-Aeppli) y Binomial negativa-Poisson (Delaporte). 

```{r Ajuste Poisson-Gaussiana inversa, echo=FALSE, results=FALSE}
fit_InvGauss <- fitdist(data = datos_agregados$N, distr = "poisinvgauss", method = "mle",  start = list(mean = 50, shape = 10))
# gofstat(fit_InvGauss, discrete = TRUE)
# cdfcomp(cex=0.8, fit_InvGauss)
```

```{r Ajuste Polya-Aeppli, echo=FALSE, results=FALSE}
library(polyaAeppli) # Composición Poisson-Geométrica
fit_PolyaAeppli <- fitdist(data = datos_agregados$N, distr = "PolyaAeppli", method = "mle",  start = list(lambda = 10, prob = 0.3), lower = c(0,0), upper = c(Inf,1))
# cdfcomp(cex=0.8, fit_PolyaAeppli)
# gofstat(fit_PolyaAeppli, discrete = TRUE)
```

```{r Delaporte, echo=FALSE, results=FALSE}
library(gamlss.dist)
fit_Delaporte <- fitdist(data = datos_agregados$N, distr = "DEL", method = "mle",  start = list(mu = 265, sigma = 75, nu = 0.6), lower = c(0,0,0), upper = c(Inf,Inf,1))

# cdfcomp(fit_Delaporte)
#  
# gofstat(fit_Delaporte, discrete = TRUE)

```

En la @fig-ajusteComp se presenta la comparación de las funciones de distribución ajustadas en comparación con la distribución empírica. Se puede observar que el ajuste no es adecuado.

```{r, echo=FALSE}
#| fig-cap: "Ajustes de la frecuencia con distribuciones compuestas"
#| label: fig-ajusteComp
#| echo: false
cdfcomp(cex=0.8, list(fit_InvGauss, fit_PolyaAeppli, fit_Delaporte), main="", xlab="Datos de frecuencia", ylab = "Función de distribución", legend=c("Poisson-Gaussiana inversa", "Polya-Aeppli", "Delaporte"), fitlty = 1)
```
Además, se halló que versiones discretas de distribuciones continuas podrían ser usadas para la modelización de la frecuencia. En este análisis nos centramos en las variaciones disponibles en el paquete \texttt{extraDistr} de \texttt{R}: la distribución Weibull discreta y la Gamma discreta. En este contexto, la discretización corresponde a que es la distribución de la parte entera de una variable aleatoria absolutamente continua. La comparación de las distribuciones teóricas ajustadas y la empírica se muestra en la figura @fig-ajusteVersionesDiscretas y se nota que estos ajustes son los mejores hasta el momento.

```{r Ajuste Weibull discreta, echo=FALSE, results=FALSE}
library(extraDistr)
fit_WeibullDiscreta <- fitdist(data = datos_agregados$N, distr = "dweibull", method = "mle",  start = list(shape1 = 0.8, shape2 = 1), lower = c(0,0), upper = c(1,Inf))

# cdfcomp(cex=0.8, fit_WeibullDiscreta)
# gofstat(fit_WeibullDiscreta, discrete = TRUE)
```

```{r Ajuste Gamma discreta, echo=FALSE, results=FALSE}
library(extraDistr)

fit_GammaDiscreta <- fitdist(data = datos_agregados$N, distr = "dgamma", method = "mle",  start = list(shape=100, scale=32), lower = c(0,0), upper = c(Inf,Inf))

# cdfcomp(fit_GammaDiscreta)
# 
# gofstat(fit_GammaDiscreta, discrete = TRUE)

```

```{r, echo=FALSE}
#| fig-cap: "Ajustes de la frecuencia con distribuciones compuestas"
#| label: fig-ajusteVersionesDiscretas
#| echo: false
cdfcomp(cex=0.8, list(fit_WeibullDiscreta, fit_GammaDiscreta), main="", xlab="Datos de frecuencia", ylab = "Función de distribución", legend=c("Weibull discreta", "Gamma discreta"), fitlty = 1)
```

Revisando el histograma en la @fig-histograma_frecuencia surge la idea de probar con mezclas de distribuciones discretas, ya que parece haber una porción hacia el inicio un poco desligada del resto.

```{r, echo=FALSE}
#| fig-cap: "Histograma de frecuencia de los reclamos por mes"
#| label: fig-histograma_frecuencia
#| echo: false
g <- ggplot(datos_agregados, aes(x=N)) 
g <- g + geom_histogram(colour="black", fill="#40A195", bins=16)
g <- g + scale_y_continuous(breaks = seq(0,22,2))
g <- g + scale_x_continuous(breaks = seq(5000,125000, 15000))
g <- g + labs(x = "Frecuencia mensual de los reclamos",
              y = "Cantidad",
              caption = "Fuente: Elaboración propia con datos de TSA")

g <- g + theme_cowplot()

g

```

Los ajustes con mezclas se limitaron a combinaciones de dos, tres, cuatro y seis distribuciones de tipo Poisson. Debe señalarse un punto muy importante respecto a estas mezclas: requieren estimar muchos parámetros. Para explicar esta aseveración, téngase en cuenta que si $p(x;\lambda_{j})$ denota la función masa de probabilidad de una distribución tipo Poisson con parámetro $\lambda_{j}$, una mezcla de $d$ distribuciones tipo Poisson tiene función masa
\[
p(x;\lambda_{1},\dots, \lambda_{d}, \alpha_{1},\dots, \alpha_{d})=\sum_{j=1}^{d}
\alpha_{j}\,p(x;\lambda_{j})
\]
para algunos pesos $\alpha_{j}$ que cumplen con ser positivos y sumar la unidad. De este último hecho, se desprende que solamente es necesario estimar $d-1$ pesos, además de los $d$ parámetros $\lambda_{j}$, sumando un total de $2d-1$ parámetros a estimar. Es decir, que con mezclas de dos, tres, cuatro y seis distribuciones tipo Poisson, deben estimarse tres, cinco, siete y once parámetros respectivamente. Como se mencionó en bitácoras anteriores, los datos de frecuencia que se busca ajustar son un total de 48, de modo que estos resultados deben tratarse con reserva, principalmente en los dos últimos casos.

En las Figuras [-@fig-ajusteMezclasPoisson23] y [-@fig-ajusteMezclasPoisson46]  se presentan estos ajustes. Visualmente, el ajuste en la @fig-ajusteMezclasPoisson23 sin duda mejora respecto a cualquier propuesta anterior y  el de la @fig-ajusteMezclasPoisson46 es muy bueno. Sin embargo, sobre todo este último ajuste debe manejarse con mucho cuidado ya que se considera que ambos modelos exhiben sobreajuste. 

```{r Funciones MLE mezcla Poisson}


mllk <- function(wpar,x){ zzz <- w2n(wpar)
        -sum(log(outer(x,zzz$lambda,dpois)%*%zzz$delta)) }

n2w  <- function(lambda,delta)log(c(lambda,delta[-1]/(1-sum(delta[-1]))))
w2n  <- function(wpar){m <- (length(wpar)+1)/2
        lambda <- exp(wpar[1:m])
        delta  <- exp(c(0,wpar[(m+1):(2*m-1)]))
return(list(lambda=lambda,delta=delta/sum(delta))) }


```

```{r Mezcla 2 Poisson, echo=FALSE}
wpar <- n2w(c(300,300),c(0.01, 0.99))
datosN <- datos_agregados$N
resultados <- w2n(nlm(mllk,wpar,datosN)$estimate)

lambda <- resultados$lambda

alpha <- resultados$delta

lambdaMezclaPoisson2<- lambda
alphaMezclaPoisson2 <- alpha

n <- length(datos_agregados$N)

loglik <- sum(log(dmixpois(x = datos_agregados$N, lambda, alpha)) )
k <- length(lambda) + length(alpha)-1
bic <- -2*loglik+k*log(n) 
aic <- -2*loglik+k*2 

fitMezclaPoisson2 <- structure(list(estimate = list(lambda = lambda, alpha=alpha),
                       method="mle", sd=NA, cor=NA, vcov=NA,
                       loglik=0, aic=aic, bic=bic, n=n, data=datos_agregados$N, fix.arg = NULL, fix.arg.fun = NULL, dots=NULL, convergence=0, discrete=TRUE,weights=NULL ,distname="mixpois"), class = "fitdist")   


gofMezclaPoisson2 <- gofstat(fitMezclaPoisson2)
gofMezclaPoisson2$chisqpvalue <- pchisq(gofMezclaPoisson2$chisq, df = length(gofMezclaPoisson2$chisqbreaks)+ 1 - 1 -length(alphaMezclaPoisson2)- length(lambdaMezclaPoisson2) + 1, lower.tail = FALSE)

```

```{r Mezcla 3 Poisson, echo=FALSE}

wpar <- n2w(c(100,200,300),c(1,1,1)/3)
resultados <- w2n(nlm(mllk,wpar,datosN)$estimate)

lambda <- resultados$lambda

alpha <- resultados$delta

lambdaMezclaPoisson3 <- lambda

alphaMezclaPoisson3 <- alpha

n <- length(datos_agregados$N)

loglik <- sum(log(dmixpois(x = datos_agregados$N, lambda, alpha)) )
k <- length(lambda) + length(alpha)-1
bic <- -2*loglik+k*log(n) 
aic <- -2*loglik+k*2 

fitMezclaPoisson3 <- structure(list(estimate = list(lambda = lambda, alpha=alpha),
                       method="mle", sd=NA, cor=NA, vcov=NA,
                       loglik=0, aic=aic, bic=bic, n=n, data=datos_agregados$N, fix.arg = NULL, fix.arg.fun = NULL, dots=NULL, convergence=0, discrete=TRUE,weights=NULL ,distname="mixpois"), class = "fitdist")   


gofMezclaPoisson3 <- gofstat(fitMezclaPoisson3)
gofMezclaPoisson3$chisqpvalue <- pchisq(gofMezclaPoisson3$chisq, df = length(gofMezclaPoisson3$chisqbreaks)+ 1 - 1 -length(alphaMezclaPoisson3)- length(lambdaMezclaPoisson3) + 1, lower.tail = FALSE)

```

```{r Mezcla 4 Poisson, echo=FALSE}
wpar <- n2w(c(90,100,90,55),c(0.1,0.4,0.3,0.1)) 

resultados <- w2n(nlm(mllk,wpar,datosN)$estimate)

lambda <- resultados$lambda

alpha <- resultados$delta

lambdaMezclaPoisson4 <- lambda

alphaMezclaPoisson4 <- alpha

n <- length(datos_agregados$N)

loglik <- sum(log(dmixpois(x = datos_agregados$N, lambda, alpha)) )
k <- length(lambda) + length(alpha)-1
bic <- -2*loglik+k*log(n) 
aic <- -2*loglik+k*2 

fitMezclaPoisson4 <- structure(list(estimate = list(lambda = lambda, alpha=alpha),
                       method="mle", sd=NA, cor=NA, vcov=NA,
                       loglik=0, aic=aic, bic=bic, n=n, data=datos_agregados$N, fix.arg = NULL, fix.arg.fun = NULL, dots=NULL, convergence=0, discrete=TRUE,weights=NULL ,distname="mixpois"), class = "fitdist")   


```

```{r Mezcla 6 Poisson, echo=FALSE}
wpar <- n2w(c(80,90,100,100,90,80),c(0.05,0.2,0.3,0.3,0.1,0.05))# 6 P


resultados <- w2n(nlm(mllk,wpar,datosN)$estimate)

lambda <- resultados$lambda

alpha <- resultados$delta

lambdaMezclaPoisson6 <- lambda

alphaMezclaPoisson6 <- alpha

n <- length(datos_agregados$N)

loglik <- sum(log(dmixpois(x = datos_agregados$N, lambda, alpha)) )
k <- length(lambda) + length(alpha)-1
bic <- -2*loglik+k*log(n) 
aic <- -2*loglik+k*2 

fitMezclaPoisson6 <- structure(list(estimate = list(lambda = lambda, alpha=alpha),
                       method="mle", sd=NA, cor=NA, vcov=NA,
                       loglik=0, aic=aic, bic=bic, n=n, data=datos_agregados$N, fix.arg = NULL, fix.arg.fun = NULL, dots=NULL, convergence=0, discrete=TRUE,weights=NULL ,distname="mixpois"), class = "fitdist")   

```



```{r}
#| fig-cap: "Ajustes de la frecuencia con mezclas de dos y tres distribuciones tipo Poisson"
#| label: fig-ajusteMezclasPoisson23
#| echo: false

cdfcomp(cex=0.8, list(fitMezclaPoisson2, fitMezclaPoisson3), main="", xlab="Datos de frecuencia", ylab = "Función de distribución", legend=c("Mezcla 2 Poisson", "Mezcla 3 Poisson"),fitcol = c("blue", "red"), fitlty = 1 )
```

```{r}
#| fig-cap: "Ajustes de la frecuencia con mezclas de cuatro y seis distribuciones tipo Poisson"
#| label: fig-ajusteMezclasPoisson46
#| echo: false
cdfcomp(cex=0.8, list(fitMezclaPoisson4, fitMezclaPoisson6), main="", xlab="Datos de frecuencia", ylab = "Función de distribución", legend=c( "Mezcla 4 Poisson", "Mezcla 6 Poisson"), datacol = c("black"), fitcol = c("blue", "red"), fitlty = 1 )


```




```{r, echo=FALSE}

dist_frec2 <- list(fit_InvGauss, fit_PolyaAeppli, fit_Delaporte,
fit_WeibullDiscreta, fit_GammaDiscreta, fitMezclaPoisson2, fitMezclaPoisson3,
fitMezclaPoisson4, fitMezclaPoisson6)


arreglo_de_metricas <- function(modelo){
par <- modelo$estimate
resultados = ""
pruebas <- gofstat(modelo, discrete = TRUE)
valor_p <- pruebas$chisqpvalue

temp <- c(modelo$distname, round(modelo$aic,2), round(modelo$bic, 2), round(valor_p, 6))

return(temp)
}

tabla <- cbind(sapply(dist_frec2, arreglo_de_metricas)) %>% t() %>% as.data.frame() 

nombres_dists <- c("Poisson-Gaussiana inversa", "Polya-Aeppli", "Delaporte",
"Weibull discreta", "Gamma discreta", "Mezcla 2 Poisson", "Mezcla 3 Poisson", "Mezcla 4 Poisson", "Mezcla 6 Poisson")

tabla[,1] <- nombres_dists

colnames(tabla) <- c("Distribución", "AIC", "BIC", "Valor $p$")

tabla$`Valor $p$`[6] <- round(gofMezclaPoisson2$chisqpvalue, 6)
tabla$`Valor $p$`[7] <- round(gofMezclaPoisson3$chisqpvalue, 6)
tabla$`Valor $p$`[8] <- ""
tabla$`Valor $p$`[9] <- ""



```


Por último, en la @tbl-MetricasNuevasBit4 Bit4 se presentan los valores $p$ de la prueba chi-cuadrado y las medidas de AIC y  BIC para los modelos nuevos ajustados para la frecuencia.


```{r}
#| tbl-cap: "Métricas de los modelos nuevos ajustados para la frecuencia"
#| label: tbl-MetricasNuevasBit4
#| echo: false

kable(tabla, format = "latex", escape = FALSE)  %>% kable_styling(latex_options = c("striped"))%>%
  kable_styling(full_width = F)%>%
  kable_classic_2() %>%
  row_spec(0,bold=TRUE)


```

### Ajuste de la severidad

```{r}
#| echo: false
#| warning: false

X <- datos_agregados$mean_sev

fit_lnorm <- fitdist(data = X, distr = "lnorm", method = "mle")

fit_exp <- fitdist(data = X, distr = "exp", method = "mle")

fit_gamma <- fitdist(data = X, distr = "gamma", method = "mle")

fit_JSU <- fitdist(data = X, distr = 'JSU', method = "mle", 
                   start=list(mu=0.3, sigma=1, nu=1,tau=0.4), 
                   lower = c(-Inf, 0,-Inf,0), upper = rep(Inf, 4) )

fit_glogis <- fitdist(data = X, distr = 'glogis', method = "mle",
                      start=list(location=0.2, scale=1, shape=1))

fit_weibull <- fitdist(data = X, distr = 'weibull', method = "mle")
```

```{r}
#| echo: false  

goftest <- gofstat(list(fit_lnorm, fit_exp, fit_gamma, 
                        fit_JSU, fit_glogis, fit_weibull))

```

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| fig-cap: "Densidad de las distribuciones ajustadas con MLE"
#| label: fig-denscompSeveridad

plot.legend <- c('lnorm', 'exp', 'gamma', 'JSU', 'gLogis', 'Weibull')
denscomp(list(fit_lnorm, fit_exp, fit_gamma, fit_JSU, fit_glogis, fit_weibull),
         main = "", ylab = 'Función de densidad', xlab = 'Datos de severidad',
         legendtext = plot.legend, cex=0.8)
```

```{r}
#| echo: false
#| fig-cap: "Función cumulativa de las distribuciones ajustadas con MLE"
#| label: fig-cdfcompSeveridad
cdfcomp(cex=0.8, list(fit_lnorm, fit_exp, fit_gamma, fit_JSU, fit_glogis, 
                      fit_weibull), main = "", xlab = 'Datos de severidad',
        ylab = 'Función de distribución', legendtext = plot.legend)
```

```{r}
#| echo: false
#| tbl-cap: "Métricas de bondad de ajuste de la severidad"
#| label: tbl-metricasSeveridad2
#| tbl-pos: 'h'

LogLik <- c(fit_lnorm$loglik, fit_exp$loglik, fit_gamma$loglik, 
            fit_JSU$loglik, fit_glogis$loglik, fit_weibull$loglik)

goft <- data.frame( dist = c('log-normal', 'exponencial', 'gamma', 
                             'Johnson SU', 'glogis', 'Weibull'), 
                    AIC = as.numeric(goftest$aic), 
                    BIC = as.numeric(goftest$bic), 
                    Loglik = LogLik, ad=as.numeric(goftest$ad), 
                    ks = as.numeric(goftest$ks))

goft %>% kbl(col.names = c('Distribución', 'AIC', 'BIC', 'LogLik', 
                'Estad. AD', 'Estad. KS'), digits = 2) %>%#| fig-cap: "Densidad de las distribuciones ajustadas con MLE"
#| label: fig-mayormontoaerolineas
                 kable_styling(latex_options = c("striped"))%>%
  kable_styling(full_width = F)%>%
  kable_classic_2() %>%
  row_spec(0,bold=TRUE)
```
Para la prueba de Kolmogorov Smirnov, una significancia del 5% corresponde a un valor crítico de 0.196299, lo que excluye la exponencial, pero todas las demás son candidatas viables. Para la prueba de Anderson-Darling una significancia del 5% corresponde a un valor crítico de 2.492 lo que nuevamente rechaza la exponencial rotundamente pero deja todas las demás distribuciones como viables. Al observar el AIC y BIC vemos que la gamma proporciona el mejor ajuste con la log-normal muy cercana también. Bajo estas medidas y el principio de parsimonía se decide elegir la gamma como la distribución de la severidad. 

### Simulación de la S

```{r}
p3 <- rmixpois(
  n = 10000,
  lambda = fitMezclaPoisson3$estimate$lambda,
  alpha = fitMezclaPoisson3$estimate$alpha
)

p4 <- rmixpois(
  n = 10000,
  lambda = fitMezclaPoisson4$estimate$lambda,
  alpha = fitMezclaPoisson4$estimate$alpha
)

p5 <- rmixpois(
  n = 10000,
  lambda = fitMezclaPoisson4$estimate$lambda,
  alpha = fitMezclaPoisson4$estimate$alpha
)

p6 <- rmixpois(
  n = 10000,
  lambda = fitMezclaPoisson4$estimate$lambda,
  alpha = fitMezclaPoisson4$estimate$alpha
)
shape=as.numeric(fit_gamma$estimate[1])
rate=as.numeric(fit_gamma$estimate[2])

S3 = rep(NA,1000); S4 = rep(NA,1000); S5 = rep(NA,1000); S6 = rep(NA,1000)
for(i in 1:1000){
  S3[i] <- sum(rgamma(p3[i],shape,rate))
  S4[i] <- sum(rgamma(p4[i],shape,rate))
  S5[i] <- sum(rgamma(p5[i],shape,rate))
  S6[i] <- sum(rgamma(p6[i],shape,rate))
}
```

```{r}
#| echo: false
#| fig-cap-location: top
#| fig-cap: Distribución de las pérdidas simuladas utilizando mixturas de Poisson
#| fig-subcap: 
#|   - "3 Poissones"
#|   - "4 Poissones"
#|   - "5 Poissones"
#|   - "6 Poissones"
#| layout-ncol: 2
#| label: fig-DistS

df <- data.frame(S3, S4, S5, S6)
ggplot(df, aes(x=S3)) + 
  geom_histogram(colour="black", fill="#40A195", bins=16)+
  xlab("Pérdidas")+ylab("Cantidad")
ggplot(df, aes(x=S4)) + 
  geom_histogram(colour="black", fill="#40A195", bins=16)+
  xlab("Pérdidas")+ylab("Cantidad")
ggplot(df, aes(x=S5)) + 
  geom_histogram(colour="black", fill="#40A195", bins=16)+
  xlab("Pérdidas")+ylab("Cantidad")
ggplot(df, aes(x=S6)) + 
  geom_histogram(colour="black", fill="#40A195", bins=16)+
  xlab("Pérdidas")+ylab("Cantidad")
```

### VaR 

```{r}
#| echo: false
#| tbl-cap: Comparación del VaR al 95% con diferentes mixturas de la Poisson
#| label: tbl-VaR
#| tbl-colwidths: [0.5,0.5,0.5,0.5,0.5]
var95Real <- quantile(datos_agregados$X,0.95)
var95P3 <- quantile(S3,0.95)
var95P4 <- quantile(S4,0.95)
var95P5 <- quantile(S5,0.95)
var95P6 <- quantile(S6,0.95)

resumen_var <- data.frame(var95Real, var95P3, var95P4, var95P5, var95P6)

resumen_var %>% kbl(
  col.names = c(
    'Empírico',
    'Poisson 3',
    'Poisson 4',
    'Poisson 5',
    'Poisson 6'
  ),
  digits = 2
) %>%
  kable_styling(latex_options = c("striped")) %>%
  kable_styling(full_width = F) %>%
  kable_classic_2() %>%
  row_spec(0, bold = TRUE)
```

Se observa que el VaR emírico con los datos reales es más alto que el VaR calculado con las simulaciones utilizando todas las mixturas de Poisson propuestas. Esto podría explicarse por el mal ajuste residual de la frecuencia y podría causar problemas en subestimación de pérdidas. 


### Ajuste del máximo


```{r}
#| echo: false  
#| warning: false
datos_maximo <- datos %>% 
  filter(disposition == "Settle" | disposition== "Approve in Full")

datos_maximo <- datos_maximo %>% 
  group_by("ano" = year(date_received),"mes" = month(date_received) ) %>% 
  summarise(max = max(close_amount))

ext <- datos_maximo$max
```


```{r}
#| echo: false
GEV <- fevd(ext, type='GEV')
```

Para ajustar las distribuciones de valores extremos primero se filtran los datos por el pago máximo realizado en cada mes. Con estos datos se procede a hacer el ajuste de la distribución de valor extremo generalizada por medio de MLE. El parámetro de forma estimado es de $\text{shape}=0.4784$, lo que sugiere que la distribución está convergiendo a una Frechet. Se procede a hacer el ajuste para las distribuciones individualmente. 

```{r}
#| echo: false

#Gumbel
Gumbel <- fevd(ext, type='Gumbel')
Gumbel_summary <- summary(Gumbel, silent=TRUE)

#Weibull
Weibull <- fitdist(ext, distr = 'weibull')

#Frechet
detach("package:extraDistr", unload = TRUE)
library(evd)
fit_frechet <- fitdist(
  ext,
  distr='frechet',
  lower=c(0,0,0), upper = c(Inf, Inf, Inf),
  start=list(loc=100, scale=100, shape=0.9)
)
```

```{r}
#| echo: false
#| fig-cap-location: top
#| fig-cap: Diagnósticos del ajuste de la distribución del máximo
#| fig-subcap: 
#|   - "pp-plot Frechet"
#|   - "pp-plot Gumbel"
#|   - "pp-plot Weibull"
#|   - "qq-plot Frechet"
#|   - "qq-plot Gumbel"
#|   - "qq-plot Weibull"
#| layout-ncol: 3
#| label: fig-diagMax

ppcomp(fit_frechet,main='')
ppcomp(Weibull,main='')
plot(Gumbel, type='probprob',col='red',main='')

qqcomp(fit_frechet,main='')
qqcomp(Weibull,main='')
plot(Gumbel, type='qq',col='red',main='')

```

```{r}
#| echo: false
#| tbl-cap: "AIC y BIC para las distribuciones de valor extremo ajustadas"
#| fig-cap-location: top
#| label: tbl-diagMax
Distribuciones <- c('Frechet', 'Gumbel', 'Weibull')
AIC <- c(fit_frechet$aic, Gumbel_summary$AIC, Weibull$aic)
BIC <- c(fit_frechet$bic, Gumbel_summary$BIC, Weibull$bic)

GOFextreme <- data.frame(Distribuciones, AIC, BIC)
GOFextreme %>% kbl(digits = 2) %>%
  kable_styling(latex_options = c("striped")) %>%
  kable_styling(full_width = F) %>%
  kable_classic_2() %>%
  row_spec(0, bold = TRUE)
```








## Nuevas fichas de resultados

9.  **Nombre de Resultado**: Uso de distribuciones compuestas para
    ajustar la frecuencia

    **Resumen en una oración**: Se probó con las composiciones
    Poisson-Gaussiana inversa, Poisson-Geométrica (Distribución
    Polya-Aeppli) y Binomial negativa-Poisson (Delaporte) y con ninguna
    se obtienen resultados satisfactorios.

    **Principal característica**: De estas tres distribuciones, la
    Polya-Aeppli mereció un mayor valor $p$ en la prueba chi-cuadrado,
    sin llegar a ser este significativo, y mejores valores de AIC y BIC.

    **Problemas o posibles desafíos**: La flexibilidad que se perseguía
    al incorporar distribuciones más complejas no se refleja en un buen
    ajuste, de modo que deben explorarse otras alternativas.

    **Resumen en un párrafo**: En la @fig-ajusteComp se presenta el
    ajuste emprendido con las composiciones Poisson-Gaussiana inversa,
    Poisson-Geométrica (Distribución Polya-Aeppli) y Binomial
    negativa-Poisson (Delaporte) con los datos de frecuencia. Aunque
    mejores que algunos ajustes con distribuciones de las familias
    $(a,b,0)$ o $(a,b,1)$, no se puede considerar el ajuste final puesto
    que está lejos de ser satisfactorio. En resumen, la flexibilidad que
    se perseguía al incorporar distribuciones más complejas no se
    refleja en un buen ajuste, de modo que deben explorarse otras
    alternativas. Cabe mencionar que de las tres distribuciones, la
    Polya-Aeppli mereció un mayor valor $p$ en la prueba chi-cuadrado,
    sin llegar a ser este significativo, y mejores valores de AIC y BIC,
    lo que se puede verificar en la @tbl-MetricasNuevasBit4 .

10. **Nombre de Resultado**: Uso de versiones discretas de
    distribuciones continuas para ajustar la frecuencia

    **Resumen en una oración**: Se probó con las versiones discretas de
    la distribuciones Weibull y gamma.

    **Principal característica**: La Weibull discreta presenta un mejor
    ajuste que cualquier otra distribución empleada, como se puede             anticipar de la @fig-ajusteVersionesDiscretas y comprobar de la            @tbl-MetricasNuevasBit4 .

    **Problemas o posibles desafíos**: Se advierten dos problemas: el
    valor p en la prueba chi-cuadrado sigue siendo bajísimo y, además,
    el parámetro $q$ de esta distribución, que cumple $0<q<1$ según se
    presentó por primera vez en @nakagawa1975discrete, es muy cercano a
    $1$ (hasta el décimo decimal hay una diferencia de aproximadamente
    $0.0000000002$) y se implementó en el paquete \texttt{extraDistr}
    [@wolodzko2019extradistr], de modo que, en la práctica, no
    resultaría útil porque no se puede evaluar en la distribución.

    **Resumen en un párrafo**: Se buscó ajustar los datos de frecuencia
    con las versiones discretas de la distribuciones Weibull y gamma. De
    estas dos, la distribución Weibull discreta presenta un mejor
    ajuste, además, respecto de cualquier otra distribución empleada
    anteriormente, como se puede anticipar de la
    @fig-ajusteVersionesDiscretas y comprobar de la
    @tbl-MetricasNuevasBit4 . No obstante, se advierten dos claros
    problemas: el valor p en la prueba chi-cuadrado sigue siendo
    bajísimo y, además, el parámetro $q$ de esta distribución, que
    cumple $0<q<1$ según se presentó por primera vez en
    @nakagawa1975discrete, es muy cercano a $1$ (hasta el décimo decimal
    hay una diferencia de aproximadamente $0.0000000002$) y se
    implementó en el paquete \texttt{extraDistr}
    [@wolodzko2019extradistr], de modo que, en la práctica, no
    resultaría útil porque no se puede evaluar en la distribución.

11. **Nombre de Resultado**: Uso de mezclas Poisson para ajustar la
    frecuencia

    **Resumen en una oración**: Se probó con mezclas de dos, tres,
    cuatro y seis distribuciones Poisson.

    **Principal característica**: Visualmente, los ajustes mejoran
    mucho, como se muestra en las Figuras [-@fig-ajusteMezclasPoisson23] y
    [-@fig-ajusteMezclasPoisson46]. Se evidencia un ajuste cada vez
    mejor conforme se aumenta la cantidad de distribuciones mezcladas.

    **Problemas o posibles desafíos**: Las mezclas tienen un nivel de
    complejidad creciente porque cada vez se requiere estimar una mayor
    cantidad de parámetros, para lo cual no se tiene una gran cantidad
    de datos (solo 48). Este hecho imposibilita una adecuada utilización
    de la prueba chi-cuadrado, ya que, con los datos de frecuencia, una
    cantidad que oscila entre los siete y ocho intervalos se requieren
    para garantizar que las frecuencias esperadas en cada intervalo sean
    iguales o superiores a cinco, lo cual constituye una regla empírica
    para el correcto funcionamiento de esta prueba estadística. De esta
    manera, con mezclas de cuatro y seis distribuciones, se tienen que
    estimar siete y once parámetros respectivamente, con lo cual, en la
    prueba chi-cuadrado de bondad de ajuste se tendrían grados de
    libertad negativos. Por esta razón, no se muestran en la
    @tbl-MetricasNuevasBit4  los valores $p$ para estos modelos en
    la prueba mencionada. En resumen, no se puede cuantificar la
    evidencia a favor de la bondad de ajuste de estos dos modelos más
    complejos con la prueba planteada y solamente se aprecia esta de
    forma gráfica.

    **Resumen en un párrafo**: Se probó con mezclas de dos, tres,
    cuatro y seis distribuciones Poisson para mejorar el ajuste de la frecuencia. Visualmente, los ajustes mejoran
    mucho, como se muestra en las Figuras [-@fig-ajusteMezclasPoisson23] y
    [-@fig-ajusteMezclasPoisson46]. Se evidencia un ajuste cada vez
    mejor conforme se aumenta la cantidad de distribuciones mezcladas.Las           mezclas tienen un nivel de
    complejidad creciente porque cada vez se requiere estimar una mayor
    cantidad de parámetros, para lo cual no se tiene una gran cantidad
    de datos (solo 48). Este hecho imposibilita una adecuada utilización
    de la prueba chi-cuadrado, ya que, con los datos de frecuencia de este          trabajo, una
    cantidad que oscila entre los siete y ocho intervalos se requieren
    para garantizar que las frecuencias esperadas en cada intervalo sean
    iguales o superiores a cinco, lo cual constituye una regla empírica
    para el correcto funcionamiento de esta prueba estadística. De esta
    manera, con mezclas de cuatro y seis distribuciones, se tienen que
    estimar siete y once parámetros respectivamente, con lo cual, en la
    prueba chi-cuadrado de bondad de ajuste se tendrían grados de
    libertad negativos. Por esta razón, no se muestran en la
    @tbl-MetricasNuevasBit4  los valores $p$ para estos modelos en
    la prueba mencionada. En resumen, no se puede cuantificar la
    evidencia a favor de la bondad de ajuste de estos dos modelos más
    complejos con la prueba planteada y solamente se aprecia esta de
    forma gráfica. Además, se prevee que se está cayendo es sobreajuste.
    
12. **Nombre de Resultado**: Ajuste de la composición de frecuencia y severidad (S)

    **Resumen en una oración**: Se hace una simulación de las pérdidas compuestas por la frecuencia y severidad y se encuentra que el promedio es similar, sin embargo parece subestimar la cola de las pérdidas. 
    
    **Principal característica**: Las distribuciones simuladas son consistentes a través de la mixturas de Poissones utilizadas y parecen tener una centralidad más a la derecha que la distribución original.
    
    **Problemas o posibles desafíos**: El VaR calculado con las simulaciones parece subestimar las empíricas con los datos originales. 
    
    **Resumen en un párrafo**: La @fig-DistS muestra las distribución de las pérdidas simuladas con cada mixtura de Poisson. Se nota consistencia entre ellos, sin embargo la forma parece ser distinta a la de los datos originales, más específicamente, parecen estar centradas más a la derecha. A pesar de que el promedio de las pérdidas simuladas y las originales es muy parecido, el VaR calculado con las simulaciones es más bajo que el empírico. Esto podría explicarse del hecho de que los datos originales muestran ocurrencias de pérdias anormalmente altas, como lo es el mes de enero del 2012.
    
    
12. **Nombre de Resultado**: Ajuste de las distribuciones de valor extremo 

    **Resumen en una oración**: Los parámetros estimados por la GEV sugieren una Frechet y la comparación del ajuste individual de la Weibull, Gumbel y Frechet apoyan este hecho 
    
    **Principal característica**: El parámetro de forma de la GEV sugiere una Frechet. 
    
    **Problemas o posibles desafíos**: Ninguno en particular 
    
    **Resumen en un párrafo**:  Los parámetros estimados por la GEV sugieren una Frechet. Luego de hacer el ajuste individual a a los datos del máximo con la Weibull, Gumbel y Frechet se observa en tbl-diagMax que el AIC y BIC apoyan el uso de la Frechet. Más evidencia para este resultado se observa en @fig-diagMax que la Frechet se acomoda mejor en los datos, especialmente los dos valores más extremos que se desvían significativamente de las distribuciones teóricas en el caso de la Gumbel y Weibull, pero que es consistente con la Frechet.  

## Parte de escritura 

### Conclusión

En esta investigación se buscaba modelar las pérdidas ligadas a los daños a la propiedad y a las personas en aeropuertos a partir de su frecuencia y severidad. La pregunta fue adoptada porque se buscaba tener un enfoque sobre el proceso de ajuste de la frecuencia y severidad con diferentes distribuciones más atípicas y con un fuerte émfasis en las pruebas de hipótesis de bondad de ajuste. Las expectivas previas eran de que el ajsute de la frecuencia sería más desafiante que el de la severidad basado en el trabajo de @flores, donde no se obtuvo un ajuste satisfactorio. El enfoque fue de hacer una composición entre la frecuencia de los reclamos y las severidades, donde cada componente fue ajustado individualmente usando MLE. 

Las distribuciones candidatas surgieron de la literatura previa, donde se hacían investigaciones con datos similares y que se encontró que la lognormal ajustaba mejor a la severidad mientas que la binomial negativa y geométrica tuvieron los mejores resultados para la frecuencia. En comparación, en este trabajo se encontró que la mixtura de tres o más poissones resultaron en el mejor ajuste para la frecuencia, siendo la de tres la preferible pues se hayaron indicios de sobreajuste para las demás, aparte de la gran cantidad de parámetros que debían incorporarse. En general se encontró que el ajuste de la frecuencia fue más complejo y requirió técnicas más avanzadas. 

Para la severidad se encontró que todas las distribuciones presentadas excepto la exponencial presentaron un ajuste satisfactorio, donde se daba un no rechazo a un nivel de significancia del 5% bajo las pruebas de Kolmogorov-Smirnov y Anderson Darling. Bajo una inspección holística de las medidas de bondad de ajuste se encontró que la gamma y la lognormal eran las más idóneas, siendo la primera la que finalmente se escogió bajo el criterio de AIC y BIC. Con la frecuencia y Severidad se llevó a cabo la composición de las pérdidas con simulaciones de tamaño mil. Se encontró que la distribución de las pérdidas parece consistente a través de todas las mixturas de Poisson intentadas y el VaR en cada caso resulta menor que el observado, posiblemente causado por el mal ajuste de la frecuencia. 

Para una extensión de este trabajo se recomienda hacer una separación de por tipo de pérdidas, y con ello hacer un ajuste individual a cada uno, similarmente a lo que se presentó en este trabajo. Finalmente, las pérdidas totales se pueden construir utilizando cópulas. Esto puede resultar en una aproximación más fina de las pérdias totales y que se expone en trabajos similares, como el de @flores. Este trabajo proporciona una ajuste de la frecuencia más elaborado que el que se hizo en la literatura relacionada, y proporcionó resultados ligeramente más satisfactorios, por lo cual llega a llenar ese vació en la metodología implementada con este tipo de datos.  

###  Reestructuración de la introducción 

Para efectos de facilidad de revisión de los distintos apartados solicitados en la  bitácora 4 para la reestructuración de la introducción, se procede a presentar esta de forma seccionada por pregunta.En la sección de ordenamiento de la bitácora  se procede a se procede presenta en formato de un solo texto, omitiendo las preguntas.

**¿Cuál es el contexto de su tema?**

El TSA (*Transportation Security Administration*) es una agencia establecida luego del 2001 que se ocupa del chequeo de los pasajeros y su equipaje en los aeropuertos de Estados Unidos.

**¿Por qué es oportuno?**

Como consecuencia de sus labores, es común que se causen daños y extravíos de las pertenencias de los pasajeros lo que resulta en reclamos por parte de los mismos en la forma de compensación monetaria por los daños ocasionados. 

**¿En qué se va a centrar específicamente en este tema?**

En este estudio nos centraremos específicamente en cuantificar las pérdidas de la agencia TSA ligadas a los reclamos antes mencionados.

**¿Qué lugar, tiempo y datos demográficos va a analizar?**  

Para esto, se va hacer uso de los datos del Departamento de Seguridad Nacional de Estados Unidos contemplando el periodo de tiempo comprendido entre el  2010 y 2013. 

**¿Cuál es la relevancia de esto en su campo académico?** 

La cuantificación de las pérdidas, es de vital importancia en cualquier empresa o institución con la finalidad de poseer las reservas o capital necesario para afrontar dichas pérdidas.

**¿Hay una relevancia política o social más amplia?**

Dada la gran frecuencia en los reclamos en aeropuertos por los daños a la propiedad y a las personas la cuantificación de  estos mediante el modelado de pérdidas agregados ha sido un tema que ha despertado el interés de diversos investigadores entre ellos @flores y @chen2020aggregate .

 @flores propone crear modelar las perdidas agregadas utilizando la base de datos que se utilizara en nuestro estudio.No obstante, si bien es cierto este autor logra ajustar de forma significativa la severidad no logra ajustar de forma adecuada la frecuencia de los reclamos.

**¿Qué nuevos conocimientos aportará, ¿Ayuda a resolver o salvar un problema teórico, aborda una laguna en la literatura, se basa en la investigación existente o propone una nueva teoría sobre su tema?**

Dada la problemática en el ajuste de la frecuencia, en este estudio se propone implementar métodos para el ajuste de esta como el uso de distribuciones compuestas, uso de versiones discretas de distribuciones continuas para ajustar la frecuencia y uso de mezclas Poisson, con la finalidad de lograr un mejor ajuste de la frecuencia de los reclamos.

**¿Cuál es el objetivo de su pregunta de investigación? ¿Qué quiere descubrir?**

En este trabajo, se busca modelar las pérdidas ligadas a los daños a la propiedad y a las personas en aeropuertos de Estados Unidos para el período 2010-2013, a partir de la frecuencia y severidad de estos.

**¿Cuál es su enfoque? ¿Cómo pretende descubrirlo?**

Para esto se propone el ajuste vía máxima verosimilitud tanto de la severidad mediante distribuciones continuas, como de las frecuencias mediante distribuciones discretas tradicionales y adicionalmente mediante los métodos alternativos antes mencionados.Por último,  la implementación de métodos de selección de modelos.

**Resumen: ¿Qué se trata en cada sección?**

El esquema del documento es el siguiente. En la sección Metodología, se describe con mayor detalle los datos utilizados y los métodos implementados para este proyecto. En la sección Resultados, se realiza un análisis de los resultados obtenidos tanto del ajuste de la frecuencia, la severidad, métodos de selección de modelos y modelo de pérdidas agregadas.En la sección  Conclusiones se presentan las conclusiones obtenidas mediante este estudio y las sugerencias para trabajos futuros.


### Resumen







### Ordenamiento de bitácora



#### Introducción 

El TSA (*Transportation Security Administration*) es una agencia establecida luego del 2001 que se ocupa del chequeo de los pasajeros y su equipaje en los aeropuertos de Estados Unidos.

Como consecuencia de sus labores, es común que se causen daños y extravíos de las pertenencias de los pasajeros lo que resulta en reclamos por parte de los mismos en la forma de compensación monetaria por los daños ocasionados.

En este estudio nos centraremos específicamente en cuantificar las pérdidas de la agencia TSA ligadas a los reclamos antes mencionados.

Para esto, se va hacer uso de los datos del Departamento de Seguridad Nacional de Estados Unidos contemplando el periodo de tiempo comprendido entre el  2010 y 2013. 

La cuantificación de las pérdidas, es de vital importancia en cualquier empresa o institución con la finalidad de poseer las reservas o capital necesario para afrontar dichas pérdidas.

Dada la gran frecuencia en los reclamos en aeropuertos por los daños a la propiedad y a las personas la cuantificación de  estos mediante el modelado de pérdidas agregados ha sido un tema que ha despertado el interés de diversos investigadores entre ellos @flores y @chen2020aggregate .

 @flores propone crear modelar las perdidas agregadas utilizando la base de datos que se utilizara en nuestro estudio.No obstante, si bien es cierto este autor logra ajustar de forma significativa la severidad no logra ajustar de forma adecuada la frecuencia de los reclamos.

Dada la problemática en el ajuste de la frecuencia, en este estudio se propone implementar métodos para el ajuste de esta como el uso de distribuciones compuestas, uso de versiones discretas de distribuciones continuas para ajustar la frecuencia y uso de mezclas Poisson, con la finalidad de lograr un mejor ajuste de la frecuencia de los reclamos.

En este trabajo, se busca modelar las pérdidas ligadas a los daños a la propiedad y a las personas en aeropuertos de Estados Unidos para el período 2010-2013, a partir de la frecuencia y severidad de estos.

Para esto se propone el ajuste vía máxima verosimilitud tanto de la severidad mediante distribuciones continuas, como de las frecuencias mediante distribuciones discretas tradicionales y adicionalmente mediante los métodos alternativos antes mencionados.Por último,  la implementación de métodos de selección de modelos.

El esquema del documento es el siguiente. En la sección Metodología, se describe con mayor detalle los datos utilizados y los métodos implementados para este proyecto. En la sección Resultados, se realiza un análisis de los resultados obtenidos tanto del ajuste de la frecuencia, la severidad, métodos de selección de modelos y modelo de pérdidas agregadas.En la sección  Conclusiones se presentan las conclusiones obtenidas mediante este estudio y las sugerencias para trabajos futuros.







