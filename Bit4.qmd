---
editor: 
  markdown: 
    wrap: 72
---

# Bitácora 4

```{r, setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
options(scipen = 99, digits = 4)

```

```{r echo=FALSE, message=FALSE, warning=TRUE}
#| output: false
#| warning: false

library(readxl)
library(kableExtra)
library(readr)
library(dplyr)
library(janitor) # función clean_names()
library(magrittr) # función %<>%
library(stringr) # función str_replace
library(lubridate)
library(PerformanceAnalytics) # funciones skewness y kurtosis
library(ggplot2)
library(actuar)
library(fitdistrplus)
library(stats)
library(cowplot) # mejorar el aspecto de los gráficos
library(gamlss) #distribucion Johnson SU
library(glogis) #distribucion logistica generalizada

#Valores Extremos
library(extRemes)
library(evd)
```

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| warning: false

datos <- read_excel("claims-2010-2013.xlsx") %>%

clean_names() # se limpian nombres columnas

datos <- datos %>% mutate(date_received = ymd(date_received),

close_amount = as.numeric(gsub("\\$", "", close_amount)))

# Se fija la base de datos

attach(datos)
```

```{r, message=FALSE, warning=FALSE}
#| echo: false

# X: Severidad

# N: Frecuencia

# Se filtran los reclamos aprobados o en los que se llegó a un acuerdo

datos_agregados <- datos %>% 
  filter(disposition == "Settle" | disposition== "Approve in Full")

datos_agregados <- datos_agregados %>% 
  group_by("ano" = year(date_received), "mes" = month(date_received) ) %>% 
  summarise(X = sum(close_amount), N = n(), mean_sev=mean(close_amount)) %>%
  ungroup() %>% mutate(t = c(1:48), .before = X)

```

## Parte de planificación

### Variciones en el ajuste de la frecuencia

En la bitácora anterior se comentó que se valoraba probar con distribuciones compuestas para mejorar el ajuste de la frecuencia, buscando con estas una mayor flexibilidad. En ese sentido se probó con las composiciones Poisson-Gaussiana inversa, Poisson-Geométrica (Distribución Polya-Aeppli) y Binomial negativa-Poisson (Delaporte). 

```{r Ajuste Poisson-Gaussiana inversa, echo=FALSE, results=FALSE}
fit_InvGauss <- fitdist(data = datos_agregados$N, distr = "poisinvgauss", method = "mle",  start = list(mean = 50, shape = 10))
# gofstat(fit_InvGauss, discrete = TRUE)
# cdfcomp(cex=0.8, fit_InvGauss)
```

```{r Ajuste Polya-Aeppli, echo=FALSE, results=FALSE}
library(polyaAeppli) # Composición Poisson-Geométrica
fit_PolyaAeppli <- fitdist(data = datos_agregados$N, distr = "PolyaAeppli", method = "mle",  start = list(lambda = 10, prob = 0.3), lower = c(0,0), upper = c(Inf,1))
# cdfcomp(cex=0.8, fit_PolyaAeppli)
# gofstat(fit_PolyaAeppli, discrete = TRUE)
```

```{r Delaporte, echo=FALSE, results=FALSE}
library(gamlss.dist)
fit_Delaporte <- fitdist(data = datos_agregados$N, distr = "DEL", method = "mle",  start = list(mu = 265, sigma = 75, nu = 0.6), lower = c(0,0,0), upper = c(Inf,Inf,1))

# cdfcomp(fit_Delaporte)
#  
# gofstat(fit_Delaporte, discrete = TRUE)

```

En la @fig-ajusteComp se presenta la comparación de las funciones de distribución ajustadas en comparación con la distribución empírica. Se puede observar que el ajuste no es adecuado.

```{r, echo=FALSE}
#| fig-cap: "Ajustes de la frecuencia con distribuciones compuestas"
#| label: fig-ajusteComp
#| echo: false
cdfcomp(cex=0.8, list(fit_InvGauss, fit_PolyaAeppli, fit_Delaporte), main="", xlab="Datos de frecuencia", ylab = "Función de distribución", legend=c("Poisson-Gaussiana inversa", "Polya-Aeppli", "Delaporte"), fitlty = 1)
```
Además, se halló que versiones discretas de distribuciones continuas podrían ser usadas para la modelización de la frecuencia. En este análisis nos centramos en las variaciones disponibles en el paquete \texttt{extraDistr} de \texttt{R}: la distribución Weibull discreta y la Gamma discreta. En este contexto, la discretización corresponde a que es la distribución de la parte entera de una variable aleatoria absolutamente continua. La comparación de las distribuciones teóricas ajustadas y la empírica se muestra en la figura @fig-ajusteVersionesDiscretas y se nota que estos ajustes son los mejores hasta el momento.

```{r Ajuste Weibull discreta, echo=FALSE, results=FALSE}
library(extraDistr)
fit_WeibullDiscreta <- fitdist(data = datos_agregados$N, distr = "dweibull", method = "mle",  start = list(shape1 = 0.8, shape2 = 1), lower = c(0,0), upper = c(1,Inf))

# cdfcomp(cex=0.8, fit_WeibullDiscreta)
# gofstat(fit_WeibullDiscreta, discrete = TRUE)
```

```{r Ajuste Gamma discreta, echo=FALSE, results=FALSE}
library(extraDistr)

fit_GammaDiscreta <- fitdist(data = datos_agregados$N, distr = "dgamma", method = "mle",  start = list(shape=100, scale=32), lower = c(0,0), upper = c(Inf,Inf))

# cdfcomp(fit_GammaDiscreta)
# 
# gofstat(fit_GammaDiscreta, discrete = TRUE)

```

```{r, echo=FALSE}
#| fig-cap: "Ajustes de la frecuencia con distribuciones compuestas"
#| label: fig-ajusteVersionesDiscretas
#| echo: false
cdfcomp(cex=0.8, list(fit_WeibullDiscreta, fit_GammaDiscreta), main="", xlab="Datos de frecuencia", ylab = "Función de distribución", legend=c("Weibull discreta", "Gamma discreta"), fitlty = 1)
```

Revisando el histograma en la @fig-histograma_frecuencia surge la idea de probar con mezclas de distribuciones discretas, ya que parece haber una porción hacia el inicio un poco desligada del resto.

```{r, echo=FALSE}
#| fig-cap: "Histograma de frecuencia de los reclamos por mes"
#| label: fig-histograma_frecuencia
#| echo: false
g <- ggplot(datos_agregados, aes(x=N)) 
g <- g + geom_histogram(colour="black", fill="#40A195", bins=16)
g <- g + scale_y_continuous(breaks = seq(0,22,2))
g <- g + scale_x_continuous(breaks = seq(5000,125000, 15000))
g <- g + labs(x = "Frecuencia mensual de los reclamos",
              y = "Cantidad",
              caption = "Fuente: Elaboración propia con datos de TSA")

g <- g + theme_cowplot()

g

```

Los ajustes con mezclas se limitaron a combinaciones de dos, tres, cuatro y seis distribuciones de tipo Poisson. Debe señalarse un punto muy importante respecto a estas mezclas: requieren estimar muchos parámetros. Para explicar esta aseveración, téngase en cuenta que si $p(x;\lambda_{j})$ denota la función masa de probabilidad de una distribución tipo Poisson con parámetro $\lambda_{j}$, una mezcla de $d$ distribuciones tipo Poisson tiene función masa
\[
p(x;\lambda_{1},\dots, \lambda_{d}, \alpha_{1},\dots, \alpha_{d})=\sum_{j=1}^{d}
\alpha_{j}\,p(x;\lambda_{j})
\]
para algunos pesos $\alpha_{j}$ que cumplen con ser positivos y sumar la unidad. De este último hecho, se desprende que solamente es necesario estimar $d-1$ pesos, además de los $d$ parámetros $\lambda_{j}$, sumando un total de $2d-1$ parámetros a estimar. Es decir, que con mezclas de dos, tres, cuatro y seis distribuciones tipo Poisson, deben estimarse tres, cinco, siete y once parámetros respectivamente. Como se mencionó en bitácoras anteriores, los datos de frecuencia que se busca ajustar son un total de 48, de modo que estos resultados deben tratarse con reserva, principalmente en los dos últimos casos.

En las Figuras [-@fig-ajusteMezclasPoisson23] y [-@fig-ajusteMezclasPoisson46]  se presentan estos ajustes. Visualmente, el ajuste en la @fig-ajusteMezclasPoisson23 sin duda mejora respecto a cualquier propuesta anterior y  el de la @fig-ajusteMezclasPoisson46 es muy bueno. Sin embargo, sobre todo este último ajuste debe manejarse con mucho cuidado ya que se considera que ambos modelos exhiben sobreajuste. 

```{r Funciones MLE mezcla Poisson}


mllk <- function(wpar,x){ zzz <- w2n(wpar)
        -sum(log(outer(x,zzz$lambda,dpois)%*%zzz$delta)) }

n2w  <- function(lambda,delta)log(c(lambda,delta[-1]/(1-sum(delta[-1]))))
w2n  <- function(wpar){m <- (length(wpar)+1)/2
        lambda <- exp(wpar[1:m])
        delta  <- exp(c(0,wpar[(m+1):(2*m-1)]))
return(list(lambda=lambda,delta=delta/sum(delta))) }


```

```{r Mezcla 2 Poisson, echo=FALSE}
wpar <- n2w(c(300,300),c(0.01, 0.99))
datosN <- datos_agregados$N
resultados <- w2n(nlm(mllk,wpar,datosN)$estimate)

lambda <- resultados$lambda

alpha <- resultados$delta

lambdaMezclaPoisson2<- lambda
alphaMezclaPoisson2 <- alpha

n <- length(datos_agregados$N)

loglik <- sum(log(dmixpois(x = datos_agregados$N, lambda, alpha)) )
k <- length(lambda) + length(alpha)-1
bic <- -2*loglik+k*log(n) 
aic <- -2*loglik+k*2 

fitMezclaPoisson2 <- structure(list(estimate = list(lambda = lambda, alpha=alpha),
                       method="mle", sd=NA, cor=NA, vcov=NA,
                       loglik=0, aic=aic, bic=bic, n=n, data=datos_agregados$N, fix.arg = NULL, fix.arg.fun = NULL, dots=NULL, convergence=0, discrete=TRUE,weights=NULL ,distname="mixpois"), class = "fitdist")   


gofMezclaPoisson2 <- gofstat(fitMezclaPoisson2)
gofMezclaPoisson2$chisqpvalue <- pchisq(gofMezclaPoisson2$chisq, df = length(gofMezclaPoisson2$chisqbreaks)+ 1 - 1 -length(alphaMezclaPoisson2)- length(lambdaMezclaPoisson2) + 1, lower.tail = FALSE)

```

```{r Mezcla 3 Poisson, echo=FALSE}

wpar <- n2w(c(100,200,300),c(1,1,1)/3)
resultados <- w2n(nlm(mllk,wpar,datosN)$estimate)

lambda <- resultados$lambda

alpha <- resultados$delta

lambdaMezclaPoisson3 <- lambda

alphaMezclaPoisson3 <- alpha

n <- length(datos_agregados$N)

loglik <- sum(log(dmixpois(x = datos_agregados$N, lambda, alpha)) )
k <- length(lambda) + length(alpha)-1
bic <- -2*loglik+k*log(n) 
aic <- -2*loglik+k*2 

fitMezclaPoisson3 <- structure(list(estimate = list(lambda = lambda, alpha=alpha),
                       method="mle", sd=NA, cor=NA, vcov=NA,
                       loglik=0, aic=aic, bic=bic, n=n, data=datos_agregados$N, fix.arg = NULL, fix.arg.fun = NULL, dots=NULL, convergence=0, discrete=TRUE,weights=NULL ,distname="mixpois"), class = "fitdist")   


gofMezclaPoisson3 <- gofstat(fitMezclaPoisson3)
gofMezclaPoisson3$chisqpvalue <- pchisq(gofMezclaPoisson3$chisq, df = length(gofMezclaPoisson3$chisqbreaks)+ 1 - 1 -length(alphaMezclaPoisson3)- length(lambdaMezclaPoisson3) + 1, lower.tail = FALSE)

```

```{r Mezcla 4 Poisson, echo=FALSE}
wpar <- n2w(c(90,100,90,55),c(0.1,0.4,0.3,0.1)) 

resultados <- w2n(nlm(mllk,wpar,datosN)$estimate)

lambda <- resultados$lambda

alpha <- resultados$delta

lambdaMezclaPoisson4 <- lambda

alphaMezclaPoisson4 <- alpha

n <- length(datos_agregados$N)

loglik <- sum(log(dmixpois(x = datos_agregados$N, lambda, alpha)) )
k <- length(lambda) + length(alpha)-1
bic <- -2*loglik+k*log(n) 
aic <- -2*loglik+k*2 

fitMezclaPoisson4 <- structure(list(estimate = list(lambda = lambda, alpha=alpha),
                       method="mle", sd=NA, cor=NA, vcov=NA,
                       loglik=0, aic=aic, bic=bic, n=n, data=datos_agregados$N, fix.arg = NULL, fix.arg.fun = NULL, dots=NULL, convergence=0, discrete=TRUE,weights=NULL ,distname="mixpois"), class = "fitdist")   


```

```{r Mezcla 6 Poisson, echo=FALSE}
wpar <- n2w(c(80,90,100,100,90,80),c(0.05,0.2,0.3,0.3,0.1,0.05))# 6 P


resultados <- w2n(nlm(mllk,wpar,datosN)$estimate)

lambda <- resultados$lambda

alpha <- resultados$delta

lambdaMezclaPoisson6 <- lambda

alphaMezclaPoisson6 <- alpha

n <- length(datos_agregados$N)

loglik <- sum(log(dmixpois(x = datos_agregados$N, lambda, alpha)) )
k <- length(lambda) + length(alpha)-1
bic <- -2*loglik+k*log(n) 
aic <- -2*loglik+k*2 

fitMezclaPoisson6 <- structure(list(estimate = list(lambda = lambda, alpha=alpha),
                       method="mle", sd=NA, cor=NA, vcov=NA,
                       loglik=0, aic=aic, bic=bic, n=n, data=datos_agregados$N, fix.arg = NULL, fix.arg.fun = NULL, dots=NULL, convergence=0, discrete=TRUE,weights=NULL ,distname="mixpois"), class = "fitdist")   

```



```{r}
#| fig-cap: "Ajustes de la frecuencia con mezclas de dos y tres distribuciones tipo Poisson"
#| label: fig-ajusteMezclasPoisson23
#| echo: false

cdfcomp(cex=0.8, list(fitMezclaPoisson2, fitMezclaPoisson3), main="", xlab="Datos de frecuencia", ylab = "Función de distribución", legend=c("Mezcla 2 Poisson", "Mezcla 3 Poisson"),fitcol = c("blue", "red"), fitlty = 1 )
```

```{r}
#| fig-cap: "Ajustes de la frecuencia con mezclas de cuatro y seis distribuciones tipo Poisson"
#| label: fig-ajusteMezclasPoisson46
#| echo: false
cdfcomp(cex=0.8, list(fitMezclaPoisson4, fitMezclaPoisson6), main="", xlab="Datos de frecuencia", ylab = "Función de distribución", legend=c( "Mezcla 4 Poisson", "Mezcla 6 Poisson"), datacol = c("black"), fitcol = c("blue", "red"), fitlty = 1 )


```




```{r, echo=FALSE}

dist_frec2 <- list(fit_InvGauss, fit_PolyaAeppli, fit_Delaporte,
fit_WeibullDiscreta, fit_GammaDiscreta, fitMezclaPoisson2, fitMezclaPoisson3,
fitMezclaPoisson4, fitMezclaPoisson6)


arreglo_de_metricas <- function(modelo){
par <- modelo$estimate
resultados = ""
pruebas <- gofstat(modelo, discrete = TRUE)
valor_p <- pruebas$chisqpvalue

temp <- c(modelo$distname, round(modelo$aic,2), round(modelo$bic, 2), round(valor_p, 6))

return(temp)
}

tabla <- cbind(sapply(dist_frec2, arreglo_de_metricas)) %>% t() %>% as.data.frame() 

nombres_dists <- c("Poisson-Gaussiana inversa", "Polya-Aeppli", "Delaporte",
"Weibull discreta", "Gamma discreta", "Mezcla 2 Poisson", "Mezcla 3 Poisson", "Mezcla 4 Poisson", "Mezcla 6 Poisson")

tabla[,1] <- nombres_dists

colnames(tabla) <- c("Distribución", "AIC", "BIC", "Valor $p$")

tabla$`Valor $p$`[6] <- round(gofMezclaPoisson2$chisqpvalue, 6)
tabla$`Valor $p$`[7] <- round(gofMezclaPoisson3$chisqpvalue, 6)
tabla$`Valor $p$`[8] <- ""
tabla$`Valor $p$`[9] <- ""



```


Por último, en la @tbl-MetricasNuevasBit4 Bit4 se presentan los valores $p$ de la prueba chi-cuadrado y las medidas de AIC y  BIC para los modelos nuevos ajustados para la frecuencia.


```{r}
#| tbl-cap: "Métricas de los modelos nuevos ajustados para la frecuencia"
#| label: tbl-MetricasNuevasBit4
#| echo: false

kable(tabla, format = "latex", escape = FALSE)  %>% kable_styling(latex_options = c("striped"))%>%
  kable_styling(full_width = F)%>%
  kable_classic_2() %>%
  row_spec(0,bold=TRUE)


```

### Ajuste de la severidad

```{r}
#| echo: false
#| warning: false

X <- datos_agregados$mean_sev/100

fit_lnorm <- fitdist(data = X, distr = "lnorm", method = "mle")

fit_exp <- fitdist(data = X, distr = "exp", method = "mle")

fit_gamma <- fitdist(data = X, distr = "gamma", method = "mle")

fit_JSU <- fitdist(data = X, distr = 'JSU', method = "mle", 
                   start=list(mu=0.3, sigma=1, nu=1,tau=0.4), 
                   lower = c(-Inf, 0,-Inf,0), upper = rep(Inf, 4) )

fit_glogis <- fitdist(data = X, distr = 'glogis', method = "mle",
                      start=list(location=0.2, scale=1, shape=1))

fit_weibull <- fitdist(data = X, distr = 'weibull', method = "mle")
```

```{r}
#| echo: false  

goftest <- gofstat(list(fit_lnorm, fit_exp, fit_gamma, 
                        fit_JSU, fit_glogis, fit_weibull))

```

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| fig-cap: "Densidad de las distribuciones ajustadas con MLE"
#| label: fig-denscompSeveridad

plot.legend <- c('lnorm', 'exp', 'gamma', 'JSU', 'gLogis', 'Weibull')
denscomp(list(fit_lnorm, fit_exp, fit_gamma, fit_JSU, fit_glogis, fit_weibull),
         main = "", ylab = 'Función de densidad', xlab = 'Datos de severidad',
         legendtext = plot.legend, cex=0.8)
```

```{r}
#| echo: false
#| fig-cap: "Función cumulativa de las distribuciones ajustadas con MLE"
#| label: fig-cdfcompSeveridad
cdfcomp(cex=0.8, list(fit_lnorm, fit_exp, fit_gamma, fit_JSU, fit_glogis, 
                      fit_weibull), main = "", xlab = 'Datos de severidad',
        ylab = 'Función de distribución', legendtext = plot.legend)
```

```{r}
#| echo: false
#| tbl-cap: "Métricas de bondad de ajuste de la severidad"
#| label: tbl-metricasSeveridad2
#| tbl-pos: 'h'

LogLik <- c(fit_lnorm$loglik, fit_exp$loglik, fit_gamma$loglik, 
            fit_JSU$loglik, fit_glogis$loglik, fit_weibull$loglik)

goft <- data.frame( dist = c('log-normal', 'exponencial', 'gamma', 
                             'Johnson SU', 'glogis', 'Weibull'), 
                    AIC = as.numeric(goftest$aic), 
                    BIC = as.numeric(goftest$bic), 
                    Loglik = LogLik, ad=as.numeric(goftest$ad), 
                    ks = as.numeric(goftest$ks))

goft %>% kbl(col.names = c('Distribución', 'AIC', 'BIC', 'LogLik', 
                'Estad. AD', 'Estad. KS'), digits = 2) %>%#| fig-cap: "Densidad de las distribuciones ajustadas con MLE"
#| label: fig-mayormontoaerolineas
                 kable_styling(latex_options = c("striped"))%>%
  kable_styling(full_width = F)%>%
  kable_classic_2() %>%
  row_spec(0,bold=TRUE)
```

Simulación de la S

```{r}
p3 <- rmixpois(
  n = 10000,
  lambda = fitMezclaPoisson3$estimate$lambda,
  alpha = fitMezclaPoisson3$estimate$alpha
)

p4 <- rmixpois(
  n = 10000,
  lambda = fitMezclaPoisson4$estimate$lambda,
  alpha = fitMezclaPoisson4$estimate$alpha
)

p5 <- rmixpois(
  n = 10000,
  lambda = fitMezclaPoisson4$estimate$lambda,
  alpha = fitMezclaPoisson4$estimate$alpha
)

p6 <- rmixpois(
  n = 10000,
  lambda = fitMezclaPoisson4$estimate$lambda,
  alpha = fitMezclaPoisson4$estimate$alpha
)

location = as.numeric(fit_glogis$estimate[1])
scale = as.numeric(fit_glogis$estimate[2])
shape = as.numeric(fit_glogis$estimate[3])

S3 = rep(NA,1000); S4 = rep(NA,1000); S5 = rep(NA,1000); S6 = rep(NA,1000)
for(i in 1:1000){
  S3[i] <- 100*sum(rglogis(p3[i],location,scale,shape))
  S4[i] <- 100*sum(rglogis(p4[i],location,scale,shape))
  S5[i] <- 100*sum(rglogis(p5[i],location,scale,shape))
  S6[i] <- 100*sum(rglogis(p6[i],location,scale,shape))
}
```

```{r}
#| layout-ncol: 2
hist(datos_agregados$X)
hist(S3)
hist(S4)
hist(S5)
hist(S6)
```
### VaR 

```{r}
var95Real <- quantile(datos_agregados$X,0.95)
var95P3 <- quantile(S3,0.95)
var95P4 <- quantile(S4,0.95)
var95P5 <- quantile(S5,0.95)
var95P6 <- quantile(S6,0.95)

resumen_var <- data.frame(var95Real, var95P3, var95P4, var95P5, var95P6)

resumen_var %>% kbl(
  col.names = c(
    'VaR 95 Empírico',
    'VaR 95 Poisson 3',
    'VaR 95 Poisson 4',
    'VaR 95 Poisson 5',
    'VaR 95 Poisson 6'
  ),
  digits = 2
) %>%
  kable_styling(latex_options = c("striped")) %>%
  kable_styling(full_width = F) %>%
  kable_classic_2() %>%
  row_spec(0, bold = TRUE)
```




### Ajuste del máximo

```{r}
#| echo: false  
#| warning: false
datos_maximo <- datos %>% 
  filter(disposition == "Settle" | disposition== "Approve in Full")

datos_maximo <- datos_maximo %>% 
  group_by("ano" = year(date_received),"mes" = month(date_received) ) %>% 
  summarise(max = max(close_amount))

ext <- datos_maximo$max
```


```{r}
#| layout-ncol: 1
#| echo: false
GEV <- fevd(ext, type='GEV')
GEV_summary <- summary(GEV, silent=TRUE)
plot(GEV)
```

```{r}
#| echo: false
Gumbel <- fevd(ext, type='Gumbel')
Gumbel_summary <- summary(Gumbel, silent=TRUE)
plot(Gumbel)
```

```{r}
#| echo: false
#| warning: false
Weibull <- fitdist(ext, distr = 'weibull')
plot(Weibull)
```

AIC 

```{r}
#| echo: false
GEV_summary$AIC
Gumbel_summary$AIC
Weibull$aic
```
BIC

```{r}
#| echo: false
GEV_summary$BIC
Gumbel_summary$BIC
Weibull$bic
```


## Nuevas fichas de resultados

9.  **Nombre de Resultado**: Uso de distribuciones compuestas para
    ajustar la frecuencia

    **Resumen en una oración**: Se probó con las composiciones
    Poisson-Gaussiana inversa, Poisson-Geométrica (Distribución
    Polya-Aeppli) y Binomial negativa-Poisson (Delaporte) y con ninguna
    se obtienen resultados satisfactorios.

    **Principal característica**: De estas tres distribuciones, la
    Polya-Aeppli mereció un mayor valor $p$ en la prueba chi-cuadrado,
    sin llegar a ser este significativo, y mejores valores de AIC y BIC.

    **Problemas o posibles desafíos**: La flexibilidad que se perseguía
    al incorporar distribuciones más complejas no se refleja en un buen
    ajuste, de modo que deben explorarse otras alternativas.

    **Resumen en un párrafo**: En la @fig-ajusteComp se presenta el
    ajuste emprendido con las composiciones Poisson-Gaussiana inversa,
    Poisson-Geométrica (Distribución Polya-Aeppli) y Binomial
    negativa-Poisson (Delaporte) con los datos de frecuencia. Aunque
    mejores que algunos ajustes con distribuciones de las familias
    $(a,b,0)$ o $(a,b,1)$, no se puede considerar el ajuste final puesto
    que está lejos de ser satisfactorio. En resumen, la flexibilidad que
    se perseguía al incorporar distribuciones más complejas no se
    refleja en un buen ajuste, de modo que deben explorarse otras
    alternativas. Cabe mencionar que de las tres distribuciones, la
    Polya-Aeppli mereció un mayor valor $p$ en la prueba chi-cuadrado,
    sin llegar a ser este significativo, y mejores valores de AIC y BIC,
    lo que se puede verificar en la @tbl-MetricasNuevasBit4 .

10. **Nombre de Resultado**: Uso de versiones discretas de
    distribuciones continuas para ajustar la frecuencia

    **Resumen en una oración**: Se probó con las versiones discretas de
    la distribuciones Weibull y gamma.

    **Principal característica**: La Weibull discreta presenta un mejor
    ajuste que cualquier otra distribución empleada, como se puede             anticipar de la @fig-ajusteVersionesDiscretas y comprobar de la            @tbl-MetricasNuevasBit4 .

    **Problemas o posibles desafíos**: Se advierten dos problemas: el
    valor p en la prueba chi-cuadrado sigue siendo bajísimo y, además,
    el parámetro $q$ de esta distribución, que cumple $0<q<1$ según se
    presentó por primera vez en @nakagawa1975discrete, es muy cercano a
    $1$ (hasta el décimo decimal hay una diferencia de aproximadamente
    $0.0000000002$) y se implementó en el paquete \texttt{extraDistr}
    [@wolodzko2019extradistr], de modo que, en la práctica, no
    resultaría útil porque no se puede evaluar en la distribución.

    **Resumen en un párrafo**: Se buscó ajustar los datos de frecuencia
    con las versiones discretas de la distribuciones Weibull y gamma. De
    estas dos, la distribución Weibull discreta presenta un mejor
    ajuste, además, respecto de cualquier otra distribución empleada
    anteriormente, como se puede anticipar de la
    @fig-ajusteVersionesDiscretas y comprobar de la
    @tbl-MetricasNuevasBit4 . No obstante, se advierten dos claros
    problemas: el valor p en la prueba chi-cuadrado sigue siendo
    bajísimo y, además, el parámetro $q$ de esta distribución, que
    cumple $0<q<1$ según se presentó por primera vez en
    @nakagawa1975discrete, es muy cercano a $1$ (hasta el décimo decimal
    hay una diferencia de aproximadamente $0.0000000002$) y se
    implementó en el paquete \texttt{extraDistr}
    [@wolodzko2019extradistr], de modo que, en la práctica, no
    resultaría útil porque no se puede evaluar en la distribución.

11. **Nombre de Resultado**: Uso de mezclas Poisson para ajustar la
    frecuencia

    **Resumen en una oración**: Se probó con mezclas de dos, tres,
    cuatro y seis distribuciones Poisson.

    **Principal característica**: Visualmente, los ajustes mejoran
    mucho, como se muestra en las Figuras [-@fig-ajusteMezclasPoisson23] y
    [-@fig-ajusteMezclasPoisson46]. Se evidencia un ajuste cada vez
    mejor conforme se aumenta la cantidad de distribuciones mezcladas.

    **Problemas o posibles desafíos**: Las mezclas tienen un nivel de
    complejidad creciente porque cada vez se requiere estimar una mayor
    cantidad de parámetros, para lo cual no se tiene una gran cantidad
    de datos (solo 48). Este hecho imposibilita una adecuada utilización
    de la prueba chi-cuadrado, ya que, con los datos de frecuencia, una
    cantidad que oscila entre los siete y ocho intervalos se requieren
    para garantizar que las frecuencias esperadas en cada intervalo sean
    iguales o superiores a cinco, lo cual constituye una regla empírica
    para el correcto funcionamiento de esta prueba estadística. De esta
    manera, con mezclas de cuatro y seis distribuciones, se tienen que
    estimar siete y once parámetros respectivamente, con lo cual, en la
    prueba chi-cuadrado de bondad de ajuste se tendrían grados de
    libertad negativos. Por esta razón, no se muestran en la
    @tbl-MetricasNuevasBit4  los valores $p$ para estos modelos en
    la prueba mencionada. En resumen, no se puede cuantificar la
    evidencia a favor de la bondad de ajuste de estos dos modelos más
    complejos con la prueba planteada y solamente se aprecia esta de
    forma gráfica.

    **Resumen en un párrafo**: Se probó con mezclas de dos, tres,
    cuatro y seis distribuciones Poisson para mejorar el ajuste de la frecuencia. Visualmente, los ajustes mejoran
    mucho, como se muestra en las Figuras [-@fig-ajusteMezclasPoisson23] y
    [-@fig-ajusteMezclasPoisson46]. Se evidencia un ajuste cada vez
    mejor conforme se aumenta la cantidad de distribuciones mezcladas.Las           mezclas tienen un nivel de
    complejidad creciente porque cada vez se requiere estimar una mayor
    cantidad de parámetros, para lo cual no se tiene una gran cantidad
    de datos (solo 48). Este hecho imposibilita una adecuada utilización
    de la prueba chi-cuadrado, ya que, con los datos de frecuencia de este          trabajo, una
    cantidad que oscila entre los siete y ocho intervalos se requieren
    para garantizar que las frecuencias esperadas en cada intervalo sean
    iguales o superiores a cinco, lo cual constituye una regla empírica
    para el correcto funcionamiento de esta prueba estadística. De esta
    manera, con mezclas de cuatro y seis distribuciones, se tienen que
    estimar siete y once parámetros respectivamente, con lo cual, en la
    prueba chi-cuadrado de bondad de ajuste se tendrían grados de
    libertad negativos. Por esta razón, no se muestran en la
    @tbl-MetricasNuevasBit4  los valores $p$ para estos modelos en
    la prueba mencionada. En resumen, no se puede cuantificar la
    evidencia a favor de la bondad de ajuste de estos dos modelos más
    complejos con la prueba planteada y solamente se aprecia esta de
    forma gráfica. Además, se prevee que se está cayendo es sobreajuste.
    

    
    
    

## Parte de escritura


